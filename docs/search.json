[
  {
    "objectID": "Homeowrk_9.html",
    "href": "Homeowrk_9.html",
    "title": "Homework_9",
    "section": "",
    "text": "Important parts from Homework 8.\n\nbike_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",\n                      local = locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbike_data &lt;- bike_data |&gt;\n  mutate(date = lubridate::dmy(Date)) |&gt;\n  select(-Date)\n\nbike_data &lt;- bike_data |&gt;\n  mutate(seasons = factor(Seasons),\n         holiday = factor(Holiday),\n         fn_day = factor(`Functioning Day`)) |&gt;\n  select(-Seasons, -Holiday, -`Functioning Day`)\n\nbike_data &lt;- bike_data |&gt;\n  rename('bike_count' = `Rented Bike Count`,\n         'hour' = \"Hour\",\n         \"temp\" = `Temperature(°C)`,\n         \"wind_speed\" = `Wind speed (m/s)`,\n         \"humidity\" = `Humidity(%)`,\n         \"vis\" = `Visibility (10m)`,\n         \"dew_point_temp\" = `Dew point temperature(°C)`,\n         \"solar_radiation\" = `Solar Radiation (MJ/m2)`,\n         \"rainfall\" = \"Rainfall(mm)\",\n         \"snowfall\" = `Snowfall (cm)`)\n\nbike_data &lt;- bike_data |&gt;\n  filter(fn_day == \"Yes\") |&gt;\n  select(-fn_day)\n\nbike_data &lt;- bike_data |&gt;\n  group_by(date, seasons, holiday) |&gt;\n  summarize(bike_count = sum(bike_count),\n            temp = mean(temp),\n            humidity = mean(humidity),\n            wind_speed = mean(wind_speed),\n            vis = mean(vis),\n            dew_point_temp = mean(dew_point_temp),\n            solar_radiation = mean(solar_radiation),\n            rainfall = sum(rainfall),\n            snowfall = sum(snowfall)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'date', 'seasons'. You can override using\nthe `.groups` argument.\n\nset.seed(11)\nbike_split &lt;- initial_split(bike_data, prop = 0.75, strata = seasons)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\nbike_10_fold &lt;- vfold_cv(bike_train, 10)\n\nChoosing best MLR, also from Homework 8.\n\nrec1 &lt;- recipe(bike_count ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt;\n  step_dummy(seasons, holiday, day_type) |&gt;\n  step_normalize(all_numeric(), -bike_count)\n\nrec2 &lt;- rec1 |&gt;\n  step_interact(terms = ~starts_with(\"seasons\")*starts_with(\"holiday\") +\n                  starts_with(\"seasons\")*temp +\n                  temp*rainfall)\n\nrec3 &lt;- rec2 |&gt;\n  step_poly(temp, wind_speed, vis, dew_point_temp, solar_radiation, rainfall, snowfall, degree = 2)\n\nMLR_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\nMLR_CV_fit1 &lt;- workflow() |&gt;\n  add_recipe(rec1) |&gt;\n  add_model(MLR_spec) |&gt;\n  fit_resamples(bike_10_fold)\nMLR_CV_fit2 &lt;- workflow() |&gt;\n  add_recipe(rec2) |&gt;\n  add_model(MLR_spec) |&gt;\n  fit_resamples(bike_10_fold)\nMLR_CV_fit3 &lt;- workflow() |&gt;\n  add_recipe(rec3) |&gt;\n  add_model(MLR_spec) |&gt;\n  fit_resamples(bike_10_fold)\n\nrbind(MLR_CV_fit1 |&gt; collect_metrics()|&gt; filter(.metric == \"rmse\"),\n      MLR_CV_fit2 |&gt; collect_metrics()|&gt; filter(.metric == \"rmse\"),\n      MLR_CV_fit3 |&gt; collect_metrics()|&gt; filter(.metric == \"rmse\"))|&gt;\n  mutate(Model = c(\"Model 1\", \"Model 2\", \"Model 3\")) |&gt;\n  select(Model, mean, n, std_err)\n\n# A tibble: 3 × 4\n  Model    mean     n std_err\n  &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 Model 1 4284.    10    165.\n2 Model 2 3156.    10    267.\n3 Model 3 3070.    10    213.\n\n\nMLR 3 has the lowest RMSE, so we apply that model to the entire data set. We’ll obtain the RMSE and the coefficient table.\n\nMLR_final &lt;- workflow() |&gt;\n  add_recipe(rec3) |&gt;\n  add_model(MLR_spec) |&gt;\n  last_fit(bike_split) \nMLR_final |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3285.    Preprocessor1_Model1\n2 rsq     standard       0.895 Preprocessor1_Model1\n\nMLR_final|&gt;extract_fit_parsnip()|&gt;tidy()\n\n# A tibble: 28 × 5\n   term                                estimate std.error statistic  p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                         22502.       1533.  14.7     4.55e-35\n 2 humidity                            -2138.       1434.  -1.49    1.37e- 1\n 3 seasons_Spring                      -1984.        238.  -8.33    6.67e-15\n 4 seasons_Summer                       8057.        921.   8.75    4.20e-16\n 5 seasons_Winter                      -4114.        994.  -4.14    4.88e- 5\n 6 holiday_No.Holiday                    894.        191.   4.69    4.61e- 6\n 7 day_type_Weekend                    -1098.        169.  -6.50    4.66e-10\n 8 seasons_Spring_x_holiday_No.Holiday    -2.41      256.  -0.00942 9.92e- 1\n 9 seasons_Summer_x_holiday_No.Holiday  -126.        239.  -0.529   5.97e- 1\n10 seasons_Winter_x_holiday_No.Holiday  -314.        188.  -1.67    9.63e- 2\n# ℹ 18 more rows\n\n\nRepeat for LASSO\n\nLASSO_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\nLASSO_wkf_1 &lt;- workflow() |&gt;\n  add_recipe(rec1) |&gt;\n  add_model(LASSO_spec)\n\nLASSO_grid_1 &lt;- LASSO_wkf_1 |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = grid_regular(penalty(), levels = 200))\n\nWarning: package 'glmnet' was built under R version 4.3.3\n\nLASSO_grid_1 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\")\n\n# A tibble: 200 × 7\n    penalty .metric .estimator  mean     n std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   4272.    10    168. Preprocessor1_Model001\n 2 1.12e-10 rmse    standard   4272.    10    168. Preprocessor1_Model002\n 3 1.26e-10 rmse    standard   4272.    10    168. Preprocessor1_Model003\n 4 1.41e-10 rmse    standard   4272.    10    168. Preprocessor1_Model004\n 5 1.59e-10 rmse    standard   4272.    10    168. Preprocessor1_Model005\n 6 1.78e-10 rmse    standard   4272.    10    168. Preprocessor1_Model006\n 7 2.00e-10 rmse    standard   4272.    10    168. Preprocessor1_Model007\n 8 2.25e-10 rmse    standard   4272.    10    168. Preprocessor1_Model008\n 9 2.52e-10 rmse    standard   4272.    10    168. Preprocessor1_Model009\n10 2.83e-10 rmse    standard   4272.    10    168. Preprocessor1_Model010\n# ℹ 190 more rows\n\nLASSO_grid_1 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line()\n\n\n\n\n\n\n\nlowest_rmse_1 &lt;- LASSO_grid_1 |&gt;\n  select_best(metric = \"rmse\")\nlowest_rmse_1\n\n# A tibble: 1 × 2\n       penalty .config               \n         &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001 Preprocessor1_Model001\n\nLASSO_wkf_1 |&gt;\n  finalize_workflow(lowest_rmse_1)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet \n\n## LASSO 2\n\nLASSO_wkf_2 &lt;- workflow() |&gt;\n  add_recipe(rec2) |&gt;\n  add_model(LASSO_spec)\n\nLASSO_grid_2 &lt;- LASSO_wkf_2 |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = grid_regular(penalty(), levels = 200))\n\nLASSO_grid_2 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\")\n\n# A tibble: 200 × 7\n    penalty .metric .estimator  mean     n std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   3077.    10    282. Preprocessor1_Model001\n 2 1.12e-10 rmse    standard   3077.    10    282. Preprocessor1_Model002\n 3 1.26e-10 rmse    standard   3077.    10    282. Preprocessor1_Model003\n 4 1.41e-10 rmse    standard   3077.    10    282. Preprocessor1_Model004\n 5 1.59e-10 rmse    standard   3077.    10    282. Preprocessor1_Model005\n 6 1.78e-10 rmse    standard   3077.    10    282. Preprocessor1_Model006\n 7 2.00e-10 rmse    standard   3077.    10    282. Preprocessor1_Model007\n 8 2.25e-10 rmse    standard   3077.    10    282. Preprocessor1_Model008\n 9 2.52e-10 rmse    standard   3077.    10    282. Preprocessor1_Model009\n10 2.83e-10 rmse    standard   3077.    10    282. Preprocessor1_Model010\n# ℹ 190 more rows\n\nLASSO_grid_2 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line()\n\n\n\n\n\n\n\nlowest_rmse_2 &lt;- LASSO_grid_2 |&gt;\n  select_best(metric = \"rmse\")\nlowest_rmse_2\n\n# A tibble: 1 × 2\n       penalty .config               \n         &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001 Preprocessor1_Model001\n\nLASSO_wkf_2 |&gt;\n  finalize_workflow(lowest_rmse_2)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n• step_interact()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet \n\n## LASSO 3\n\nLASSO_wkf_3 &lt;- workflow() |&gt;\n  add_recipe(rec3) |&gt;\n  add_model(LASSO_spec)\n\nLASSO_grid_3 &lt;- LASSO_wkf_3 |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = grid_regular(penalty(), levels = 200))\n\nLASSO_grid_3 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\")\n\n# A tibble: 200 × 7\n    penalty .metric .estimator  mean     n std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   3008.    10    228. Preprocessor1_Model001\n 2 1.12e-10 rmse    standard   3008.    10    228. Preprocessor1_Model002\n 3 1.26e-10 rmse    standard   3008.    10    228. Preprocessor1_Model003\n 4 1.41e-10 rmse    standard   3008.    10    228. Preprocessor1_Model004\n 5 1.59e-10 rmse    standard   3008.    10    228. Preprocessor1_Model005\n 6 1.78e-10 rmse    standard   3008.    10    228. Preprocessor1_Model006\n 7 2.00e-10 rmse    standard   3008.    10    228. Preprocessor1_Model007\n 8 2.25e-10 rmse    standard   3008.    10    228. Preprocessor1_Model008\n 9 2.52e-10 rmse    standard   3008.    10    228. Preprocessor1_Model009\n10 2.83e-10 rmse    standard   3008.    10    228. Preprocessor1_Model010\n# ℹ 190 more rows\n\nLASSO_grid_3 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line()\n\n\n\n\n\n\n\nlowest_rmse_3 &lt;- LASSO_grid_3 |&gt;\n  select_best(metric = \"rmse\")\nlowest_rmse_3\n\n# A tibble: 1 × 2\n       penalty .config               \n         &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001 Preprocessor1_Model001\n\nLASSO_wkf_3 |&gt;\n  finalize_workflow(lowest_rmse_3)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n7 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n• step_interact()\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet \n\n## Choose LASSO\n\nchoose_LASSO_1&lt;-LASSO_grid_1 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\")|&gt;\n  arrange(mean)\n\nchoose_LASSO_2&lt;-LASSO_grid_2 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\")|&gt;\n  arrange(mean)\n\nchoose_LASSO_3&lt;-LASSO_grid_3 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\")|&gt;\n  arrange(mean)\n\nrbind(choose_LASSO_1[1,],choose_LASSO_2[1,],choose_LASSO_3[1,])\n\n# A tibble: 3 × 7\n       penalty .metric .estimator  mean     n std_err .config               \n         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001 rmse    standard   4272.    10    168. Preprocessor1_Model001\n2 0.0000000001 rmse    standard   3077.    10    282. Preprocessor1_Model001\n3 0.0000000001 rmse    standard   3008.    10    228. Preprocessor1_Model001\n\n\nLASSO 3 has the lowest RMSE, so we apply that model to the entire data set. We’ll obtain the RMSE and the coefficient table.\n\nLASSO_wkf_3 |&gt;\n  finalize_workflow(lowest_rmse_3) |&gt;\n  last_fit(bike_split) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3286.    Preprocessor1_Model1\n2 rsq     standard       0.895 Preprocessor1_Model1\n\nLASSO_final_3 &lt;- LASSO_wkf_3 |&gt;\n  finalize_workflow(lowest_rmse_3) |&gt;\n  fit(bike_train)\ntidy(LASSO_final_3)\n\n# A tibble: 28 × 3\n   term                                estimate      penalty\n   &lt;chr&gt;                                  &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)                          23002.  0.0000000001\n 2 humidity                              -695.  0.0000000001\n 3 seasons_Spring                       -1974.  0.0000000001\n 4 seasons_Summer                        8139.  0.0000000001\n 5 seasons_Winter                       -3703.  0.0000000001\n 6 holiday_No.Holiday                     854.  0.0000000001\n 7 day_type_Weekend                     -1100.  0.0000000001\n 8 seasons_Spring_x_holiday_No.Holiday    -15.5 0.0000000001\n 9 seasons_Summer_x_holiday_No.Holiday    -99.8 0.0000000001\n10 seasons_Winter_x_holiday_No.Holiday   -269.  0.0000000001\n# ℹ 18 more rows\n\n\nRepeat for Regression Tree Model. Regression Trees inherently includes interaction terms, so we’ll only compare Recipe 1 and Recipe 3.\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\ntree_wkf_1 &lt;- workflow() |&gt;\n  add_recipe(rec1) |&gt;\n  add_model(tree_mod)\n\ntree_grid_1 &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10, 5))\n\ntree_fits_1 &lt;- tree_wkf_1 |&gt; \n  tune_grid(resamples = bike_10_fold,\n            grid = tree_grid_1)\n\ntree_fits_1 |&gt;\n  collect_metrics()\n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric .estimator     mean     n  std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 rmse    standard   6432.       10 350.     Prepro…\n 2    0.0000000001          1 rsq     standard      0.595    10   0.0484 Prepro…\n 3    0.000000001           1 rmse    standard   6432.       10 350.     Prepro…\n 4    0.000000001           1 rsq     standard      0.595    10   0.0484 Prepro…\n 5    0.00000001            1 rmse    standard   6432.       10 350.     Prepro…\n 6    0.00000001            1 rsq     standard      0.595    10   0.0484 Prepro…\n 7    0.0000001             1 rmse    standard   6432.       10 350.     Prepro…\n 8    0.0000001             1 rsq     standard      0.595    10   0.0484 Prepro…\n 9    0.000001              1 rmse    standard   6432.       10 350.     Prepro…\n10    0.000001              1 rsq     standard      0.595    10   0.0484 Prepro…\n# ℹ 90 more rows\n\ntree_fits_1 %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\ntree_fits_1 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1    0.001                11 rmse    standard   3817.    10    285. Preprocess…\n 2    0.001                15 rmse    standard   3817.    10    285. Preprocess…\n 3    0.0000000001         11 rmse    standard   3837.    10    302. Preprocess…\n 4    0.000000001          11 rmse    standard   3837.    10    302. Preprocess…\n 5    0.00000001           11 rmse    standard   3837.    10    302. Preprocess…\n 6    0.0000001            11 rmse    standard   3837.    10    302. Preprocess…\n 7    0.000001             11 rmse    standard   3837.    10    302. Preprocess…\n 8    0.00001              11 rmse    standard   3837.    10    302. Preprocess…\n 9    0.0001               11 rmse    standard   3837.    10    302. Preprocess…\n10    0.0000000001         15 rmse    standard   3837.    10    302. Preprocess…\n# ℹ 40 more rows\n\ntree_best_params_1 &lt;- tree_fits_1 |&gt;\n  select_best(metric = \"rmse\")\ntree_best_params_1\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1           0.001         11 Preprocessor1_Model38\n\ntree_final_wkf_1 &lt;- tree_wkf_1 |&gt;\n  finalize_workflow(tree_best_params_1)\n\ntree_wkf_2 &lt;- workflow() |&gt;\n  add_recipe(rec1 |&gt;\n               step_poly(temp, wind_speed, vis, dew_point_temp, solar_radiation, rainfall, snowfall, degree = 2)) |&gt;\n  add_model(tree_mod)\n\ntree_grid_2 &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10, 5))\n\ntree_fits_2 &lt;- tree_wkf_2 |&gt; \n  tune_grid(resamples = bike_10_fold,\n            grid = tree_grid_2)\n\ntree_fits_2 |&gt;\n  collect_metrics()\n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric .estimator     mean     n  std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 rmse    standard   6432.       10 350.     Prepro…\n 2    0.0000000001          1 rsq     standard      0.595    10   0.0484 Prepro…\n 3    0.000000001           1 rmse    standard   6432.       10 350.     Prepro…\n 4    0.000000001           1 rsq     standard      0.595    10   0.0484 Prepro…\n 5    0.00000001            1 rmse    standard   6432.       10 350.     Prepro…\n 6    0.00000001            1 rsq     standard      0.595    10   0.0484 Prepro…\n 7    0.0000001             1 rmse    standard   6432.       10 350.     Prepro…\n 8    0.0000001             1 rsq     standard      0.595    10   0.0484 Prepro…\n 9    0.000001              1 rmse    standard   6432.       10 350.     Prepro…\n10    0.000001              1 rsq     standard      0.595    10   0.0484 Prepro…\n# ℹ 90 more rows\n\ntree_fits_2 %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\n\n\n\n\n\n\ntree_fits_2 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1    0.001                11 rmse    standard   3846.    10    283. Preprocess…\n 2    0.001                15 rmse    standard   3846.    10    283. Preprocess…\n 3    0.0000000001         11 rmse    standard   3874.    10    296. Preprocess…\n 4    0.000000001          11 rmse    standard   3874.    10    296. Preprocess…\n 5    0.00000001           11 rmse    standard   3874.    10    296. Preprocess…\n 6    0.0000001            11 rmse    standard   3874.    10    296. Preprocess…\n 7    0.000001             11 rmse    standard   3874.    10    296. Preprocess…\n 8    0.00001              11 rmse    standard   3874.    10    296. Preprocess…\n 9    0.0001               11 rmse    standard   3874.    10    296. Preprocess…\n10    0.0000000001         15 rmse    standard   3874.    10    296. Preprocess…\n# ℹ 40 more rows\n\ntree_best_params_2 &lt;- tree_fits_2 |&gt;\n  select_best(metric = \"rmse\")\ntree_best_params_2\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1           0.001         11 Preprocessor1_Model38\n\ntree_final_wkf_2 &lt;- tree_wkf_2 |&gt;\n  finalize_workflow(tree_best_params_2)\n\n## Choose best regression tree\n\nchoose_rt_1&lt;-tree_fits_1 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\nchoose_rt_2&lt;-tree_fits_2 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\nrbind(choose_rt_1[1,], choose_rt_2[1,])\n\n# A tibble: 2 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1           0.001         11 rmse    standard   3817.    10    285. Preprocesso…\n2           0.001         11 rmse    standard   3846.    10    283. Preprocesso…\n\n\nRegression Tree 1 has the lowest RMSE, so we apply that model to the entire data set. We’ll obtain the RMSE and a plot of the final fit.\n\ntree_final_fit_1 &lt;- tree_final_wkf_1 |&gt;\n  last_fit(bike_split)\n\ntree_final_fit_1 |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3096.    Preprocessor1_Model1\n2 rsq     standard       0.905 Preprocessor1_Model1\n\ntree_final_model_1 &lt;- extract_workflow(tree_final_fit_1)\ntree_final_model_1 %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot::rpart.plot(roundint = FALSE)\n\n\n\n\n\n\n\n\nRepeat with Bagged Tree Model.\n\nbag_spec &lt;- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nbag_wkf_1 &lt;- workflow() |&gt;\n  add_recipe(rec1) |&gt;\n  add_model(bag_spec)\n\nbag_fit_1 &lt;- bag_wkf_1 |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = grid_regular(cost_complexity(),\n                                levels = 15))\n\nbag_best_params_1 &lt;- bag_fit_1 |&gt;\n  select_best(metric = \"rmse\")\nbag_best_params_1\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1   0.00000000193 Preprocessor1_Model03\n\nbag_wkf_2 &lt;- workflow() |&gt;\n  add_recipe(rec2) |&gt;\n  add_model(bag_spec)\n\nbag_fit_2 &lt;- bag_wkf_2 |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = grid_regular(cost_complexity(),\n                                levels = 15))\n\nbag_best_params_2 &lt;- bag_fit_2 |&gt;\n  select_best(metric = \"rmse\")\nbag_best_params_2\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1        4.39e-10 Preprocessor1_Model02\n\nbag_wkf_3 &lt;- workflow() |&gt;\n  add_recipe(rec3) |&gt;\n  add_model(bag_spec)\n\nbag_fit_3 &lt;- bag_wkf_3 |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = grid_regular(cost_complexity(),\n                                levels = 15))\n\nbag_best_params_3 &lt;- bag_fit_3 |&gt;\n  select_best(metric = \"rmse\")\nbag_best_params_3\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1      0.00000316 Preprocessor1_Model08\n\n## Choose best bagged\nchoose_bagged_1&lt;-bag_fit_1 |&gt;\n  collect_metrics()|&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\nchoose_bagged_2&lt;-bag_fit_2 |&gt;\n  collect_metrics()|&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\nchoose_bagged_3&lt;-bag_fit_3 |&gt;\n  collect_metrics()|&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\nrbind(choose_bagged_1[1,], choose_bagged_2[1,], choose_bagged_3[1,])\n\n# A tibble: 3 × 7\n  cost_complexity .metric .estimator  mean     n std_err .config              \n            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1        1.93e- 9 rmse    standard   3208.    10    196. Preprocessor1_Model03\n2        4.39e-10 rmse    standard   3230.    10    153. Preprocessor1_Model02\n3        3.16e- 6 rmse    standard   3318.    10    199. Preprocessor1_Model08\n\n\nBagged 2 model is the best, so we apply that model to the entire data set. We’ll obtain the RMSE and a variable importance plot.\n\nbag_final_wkf &lt;- bag_wkf_2 |&gt;\n finalize_workflow(bag_best_params_2)\nbag_final_fit &lt;- bag_final_wkf |&gt;\n last_fit(bike_split)\nbag_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3095.    Preprocessor1_Model1\n2 rsq     standard       0.907 Preprocessor1_Model1\n\nbag_final_model &lt;- bag_final_fit |&gt; extract_fit_engine()\nbag_final_model$imp |&gt;\n mutate(term = factor(term, levels = term)) |&gt;\n ggplot(aes(x = term, y = value)) +\n geom_bar(stat =\"identity\") +\n coord_flip()\n\n\n\n\n\n\n\n\nRepeat with Random Forest Model.\n\nrf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n  set_engine(\"ranger\", importance = 'impurity') |&gt;\n  set_mode(\"regression\")\n\n\nrf_wkf_1 &lt;- workflow() |&gt;\n  add_recipe(rec1) |&gt;\n  add_model(rf_spec)\n\nrf_fit_1 &lt;- rf_wkf_1 |&gt;\n  tune_grid(resamples = bike_10_fold)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nrf_fit_1 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 8 × 7\n   mtry .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1    13 rmse    standard   2993.    10    199. Preprocessor1_Model2\n2    11 rmse    standard   3025.    10    196. Preprocessor1_Model5\n3     9 rmse    standard   3034.    10    199. Preprocessor1_Model7\n4     6 rmse    standard   3077.    10    197. Preprocessor1_Model3\n5     7 rmse    standard   3079.    10    200. Preprocessor1_Model1\n6     4 rmse    standard   3148.    10    184. Preprocessor1_Model4\n7     3 rmse    standard   3209.    10    179. Preprocessor1_Model6\n8     2 rmse    standard   3477.    10    167. Preprocessor1_Model8\n\nrf_best_params_1 &lt;- rf_fit_1 |&gt;\n  select_best(metric = \"rmse\")\nrf_best_params_1\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1    13 Preprocessor1_Model2\n\nrf_wkf_2 &lt;- workflow() |&gt;\n  add_recipe(rec2) |&gt;\n  add_model(rf_spec)\n\nrf_fit_2 &lt;- rf_wkf_2 |&gt;\n  tune_grid(resamples = bike_10_fold)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nrf_fit_2 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 10 × 7\n    mtry .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1    14 rmse    standard   3084.    10    185. Preprocessor1_Model04\n 2    18 rmse    standard   3086.    10    177. Preprocessor1_Model06\n 3    11 rmse    standard   3087.    10    185. Preprocessor1_Model03\n 4     9 rmse    standard   3097.    10    186. Preprocessor1_Model01\n 5    15 rmse    standard   3100.    10    182. Preprocessor1_Model10\n 6    19 rmse    standard   3101.    10    179. Preprocessor1_Model09\n 7     8 rmse    standard   3120.    10    173. Preprocessor1_Model07\n 8     6 rmse    standard   3137.    10    175. Preprocessor1_Model08\n 9     5 rmse    standard   3164.    10    186. Preprocessor1_Model05\n10     2 rmse    standard   3436.    10    171. Preprocessor1_Model02\n\nrf_best_params_2 &lt;- rf_fit_2 |&gt;\n  select_best(metric = \"rmse\")\nrf_best_params_2\n\n# A tibble: 1 × 2\n   mtry .config              \n  &lt;int&gt; &lt;chr&gt;                \n1    14 Preprocessor1_Model04\n\nrf_wkf_3 &lt;- workflow() |&gt;\n  add_recipe(rec3) |&gt;\n  add_model(rf_spec)\n\nrf_fit_3 &lt;- rf_wkf_3 |&gt;\n  tune_grid(resamples = bike_10_fold)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nrf_fit_3 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 10 × 7\n    mtry .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1    23 rmse    standard   3145.    10    178. Preprocessor1_Model09\n 2    16 rmse    standard   3160.    10    177. Preprocessor1_Model01\n 3    27 rmse    standard   3166.    10    174. Preprocessor1_Model07\n 4    21 rmse    standard   3167.    10    187. Preprocessor1_Model02\n 5    17 rmse    standard   3170.    10    185. Preprocessor1_Model05\n 6    11 rmse    standard   3193.    10    174. Preprocessor1_Model04\n 7    12 rmse    standard   3193.    10    184. Preprocessor1_Model03\n 8     6 rmse    standard   3254.    10    175. Preprocessor1_Model08\n 9     4 rmse    standard   3296.    10    172. Preprocessor1_Model10\n10     3 rmse    standard   3371.    10    171. Preprocessor1_Model06\n\nrf_best_params_3 &lt;- rf_fit_3 |&gt;\n  select_best(metric = \"rmse\")\nrf_best_params_3\n\n# A tibble: 1 × 2\n   mtry .config              \n  &lt;int&gt; &lt;chr&gt;                \n1    23 Preprocessor1_Model09\n\n## Choose best random\nchoose_random_1&lt;-rf_fit_1 |&gt;\n  collect_metrics()|&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\nchoose_random_2&lt;-rf_fit_2 |&gt;\n  collect_metrics()|&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\nchoose_random_3&lt;-rf_fit_3 |&gt;\n  collect_metrics()|&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\nrbind(choose_random_1[1,], choose_random_2[1,], choose_random_3[1,])\n\n# A tibble: 3 × 7\n   mtry .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    13 rmse    standard   2993.    10    199. Preprocessor1_Model2 \n2    14 rmse    standard   3084.    10    185. Preprocessor1_Model04\n3    23 rmse    standard   3145.    10    178. Preprocessor1_Model09\n\n\nRandom Forest 1 model is the best, so we apply that model to the entire data set. We’ll obtain the RMSE and a variable importance plot.\n\nrandom_forest_final_wkf &lt;- rf_wkf_1 |&gt;\n  finalize_workflow(rf_best_params_1)\nrandom_forest_final_fit &lt;- random_forest_final_wkf  |&gt;\n  last_fit(bike_split)\nrandom_forest_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    2650.    Preprocessor1_Model1\n2 rsq     standard       0.933 Preprocessor1_Model1\n\nrandom_forest_final_model &lt;- random_forest_final_fit |&gt; extract_fit_engine()\naa&lt;-random_forest_final_model$variable.importance\na&lt;-attributes(aa)\naaa&lt;-data.frame(c(aa))\naaa$term&lt;-row.names(aaa)\naaa$value&lt;-aaa$c.aa.\naaa|&gt;\n  mutate(term = factor(term, levels = term)) |&gt;\n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat =\"identity\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nNow we are going to compare the best model from each model family, to find our best overall model.\n\nrbind(MLR_final |&gt; collect_metrics(), \n      LASSO_wkf_3 |&gt; finalize_workflow(lowest_rmse_3) |&gt; last_fit(bike_split) |&gt; collect_metrics(),\n      tree_final_fit_1 |&gt; collect_metrics(),\n      bag_wkf_2 |&gt;finalize_workflow(bag_best_params_2)|&gt;last_fit(bike_split)|&gt; collect_metrics(),\n      rf_wkf_1 |&gt;finalize_workflow(rf_best_params_1)|&gt;last_fit(bike_split)|&gt; collect_metrics())\n\n# A tibble: 10 × 4\n   .metric .estimator .estimate .config             \n   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n 1 rmse    standard    3285.    Preprocessor1_Model1\n 2 rsq     standard       0.895 Preprocessor1_Model1\n 3 rmse    standard    3286.    Preprocessor1_Model1\n 4 rsq     standard       0.895 Preprocessor1_Model1\n 5 rmse    standard    3096.    Preprocessor1_Model1\n 6 rsq     standard       0.905 Preprocessor1_Model1\n 7 rmse    standard    2848.    Preprocessor1_Model1\n 8 rsq     standard       0.921 Preprocessor1_Model1\n 9 rmse    standard    2596.    Preprocessor1_Model1\n10 rsq     standard       0.936 Preprocessor1_Model1\n\n\nThe Random Forest model is the best overall model.\n\nrf_final_wkf &lt;- rf_wkf_1 |&gt;\n finalize_workflow(rf_best_params_1)\nrf_final_fit &lt;- rf_final_wkf |&gt;\n last_fit(bike_split)\n\nrf_full_fit &lt;- rf_final_wkf |&gt;\n fit(bike_data)\nrf_full_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~13L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      353 \nNumber of independent variables:  13 \nMtry:                             13 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       7523227 \nR squared (OOB):                  0.9238133"
  }
]